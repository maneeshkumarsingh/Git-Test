import datetimefrom sklearn.cluster import KMeansimport pickleimport jsonimport reimport pandas as pdimport numpy as npimport timefrom sklearn.metrics.pairwise import cosine_similarityfrom datetime import datetimefrom sklearn.feature_extraction.text import TfidfVectorizerimport jsonfrom floc_simhash import SimHashhasher = SimHash(n_bits=128)from scipy import spatialimport numpy as npfrom numpy.linalg import norm# disable chained assignmentspd.options.mode.chained_assignment = Nonecurrent_time = datetime.now()with open('A2P_P2P_Config.json') as config_file:    config = json.load(config_file)percentage_match = config['PERCENTAGE_MATCH']def clean_texts(data):    corpus = []    for i in range(0, len(data)):        review = re.sub('[^a-zA-Z]', ' ', data['user_data'][i])        review = review.lower()        review = review.split()        #review = [ls.lemmatize(word) for word in review]        review = ' '.join(review)        corpus.append(review)    return corpusdef calculate_cosine_distance(a, b):    cosine_distance = float(spatial.distance.cosine(a, b))    return cosine_distancedef calculate_cosine_similarity(A, B):    cosine_similarity =  np.dot(A,B)/(norm(A)*norm(B))    return cosine_similaritydef Cosine_similarity(data1, data2):    column = list(data1.columns)    data1['length'] = data1['user_data'].str.len()    data2['length'] = data2['user_data'].str.len()    #cleaned_data = clean_texts(data)    vectorizer = TfidfVectorizer(max_features=2500)    X = vectorizer.fit_transform(data1['user_data'])    # X.shape    df1 = pd.DataFrame(X.toarray())    temp_data1 = pd.concat([df1, data1], axis=1)    #temp_data.dropna(inplace=True)    temp_data1 = temp_data1[temp_data1.length != 0]    temp_data1.reset_index(drop=True, inplace=True)    # del temp_data['index']    y = X.shape[1]    A = vectorizer.fit_transform(data2['user_data'].values.astype('U'))    # X.shape    df2 = pd.DataFrame(A.toarray())    temp_data2 = pd.concat([df2, data2], axis=1)    # temp_data.dropna(inplace=True)    temp_data2 = temp_data2[temp_data2.length != 0]    temp_data2.reset_index(drop=True, inplace=True)    # del temp_data['index']    B = A.shape[1]    i = 0    while i < len(temp_data1):        embedded_data1 = temp_data1.iloc[:, 0:y]        #len(temp_data1.user_data.iloc[i])        lists = []        j = 0        temp = temp_data2.loc[(temp_data2['length'] > len(temp_data1.user_data.iloc[i]) - 2) & (temp_data2['length'] < len(temp_data1.user_data.iloc[i]) + 2)]        if temp.empty:            i = i+1            continue        else:            temp.reset_index(drop=True, inplace=True)            embedded_data2 = temp.iloc[:, 0:B]            #temp['cosine_value'] = cosine_similarity(embedded_data2, embedded_data1.loc[i:i])#.reshape(-1)#, dense_output=False)  # Here I assume that the parent vector is stored as the first row in the dataframe, but you could also store it separately            while j < len(temp):                lists.append(calculate_cosine_similarity(embedded_data2.iloc[j], embedded_data1.loc[i:i]))                j = j + 1            y_value = pd.DataFrame(lists, columns=['cosine_value'])            dummy_data = pd.concat([temp, y_value], axis=1)            similer_data = dummy_data.loc[dummy_data['cosine_value'] > percentage_match]            similer_data.reset_index(drop=True, inplace=True)            #TOP_similer_data = similer_data.loc[similer_data['cosine_value'] > 0.99]            #TOP_similer_data = TOP_similer_data.drop_duplicates(subset='cosine_value', keep='first')            similer_data.drop(['cosine_value'], axis=1, inplace=True)            #TOP_similer_data = TOP_similer_data.drop_duplicates(subset='cosine_value', keep='first')            #TOP_similer_data.reset_index(drop=True, inplace=True)            #final_data = pd.concat([TOP_similer_data, final_data], ignore_index=True, axis=0)            #final_data.reset_index(drop=True, inplace=True)            # temp_data = temp_data - unsimiler_data            temp_data2 = temp_data2[~temp_data2.isin(similer_data)].dropna()            temp_data2.reset_index(drop=True, inplace=True)            i = i +1    #final_data['hash_value'] = final_data['user_data'].apply(lambda x: hasher.hash(x))    #final_data = final_data.drop(['cosine_value'], axis=1)    #final_data = pd.DataFrame()    final_data = pd.concat([temp_data1, temp_data2])    final_data.dropna(axis=0, inplace=True)    final_data.reset_index(drop=True, inplace=False)    #final_data['date_time'] = pd.to_datetime('now')    #final_data['date_time'] = current_time    final_data = final_data[column]    return final_data#if __name__ == '__main__': #   Cosine_similarity(data)